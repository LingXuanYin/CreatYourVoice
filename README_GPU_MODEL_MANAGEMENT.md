# GPUæ¨¡å‹ç®¡ç†ç³»ç»Ÿ

CreatYourVoiceçš„GPUæ¨¡å‹ç®¡ç†ç³»ç»Ÿæä¾›äº†ç»Ÿä¸€çš„æ¨¡å‹åŠ è½½ã€å¸è½½ã€å†…å­˜ç›‘æ§å’Œç”Ÿå‘½å‘¨æœŸç®¡ç†åŠŸèƒ½ã€‚

## ğŸ¯ æ ¸å¿ƒç‰¹æ€§

### 1. ç»Ÿä¸€æ¨¡å‹ç®¡ç†
- **å¤šæ¨¡å‹æ”¯æŒ**ï¼šåŒæ—¶ç®¡ç†DDSP-SVCå’ŒIndexTTSæ¨¡å‹
- **æ™ºèƒ½åŠ è½½**ï¼šè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜GPUè®¾å¤‡
- **å¼•ç”¨è®¡æ•°**ï¼šåŸºäºä½¿ç”¨æƒ…å†µè‡ªåŠ¨ç®¡ç†æ¨¡å‹ç”Ÿå‘½å‘¨æœŸ
- **å†…å­˜ä¼°ç®—**ï¼šé¢„ä¼°æ¨¡å‹å†…å­˜éœ€æ±‚ï¼Œé¿å…OOMé”™è¯¯

### 2. å®æ—¶å†…å­˜ç›‘æ§
- **GPUçŠ¶æ€ç›‘æ§**ï¼šå®æ—¶æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨ã€åˆ©ç”¨ç‡ã€æ¸©åº¦
- **ç³»ç»Ÿå†…å­˜ç›‘æ§**ï¼šç›‘æ§ç³»ç»ŸRAMä½¿ç”¨æƒ…å†µ
- **å†å²è®°å½•**ï¼šä¿å­˜å†…å­˜ä½¿ç”¨å†å²ç”¨äºåˆ†æ
- **æ™ºèƒ½è­¦å‘Š**ï¼šå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜æ—¶è‡ªåŠ¨è­¦å‘Š

### 3. è‡ªåŠ¨åŒ–ç®¡ç†
- **è‡ªåŠ¨æ¸…ç†**ï¼šç©ºé—²æ¨¡å‹è‡ªåŠ¨å¸è½½é‡Šæ”¾å†…å­˜
- **æ™ºèƒ½é¢„åŠ è½½**ï¼šæ ¹æ®ä»»åŠ¡éœ€æ±‚é¢„åŠ è½½æ¨¡å‹
- **å†…å­˜ä¼˜åŒ–**ï¼šè‡ªåŠ¨æ¸…ç†GPUç¼“å­˜å’Œåƒåœ¾å›æ”¶
- **é”™è¯¯æ¢å¤**ï¼šæ¨¡å‹åŠ è½½å¤±è´¥æ—¶è‡ªåŠ¨é‡è¯•æˆ–é™çº§

### 4. ç”¨æˆ·å‹å¥½ç•Œé¢
- **ç›´è§‚æ§åˆ¶**ï¼šä¸€é”®åŠ è½½/å¸è½½æ¨¡å‹
- **å®æ—¶çŠ¶æ€**ï¼šæ˜¾ç¤ºæ¨¡å‹çŠ¶æ€å’Œå†…å­˜ä½¿ç”¨
- **æ‰¹é‡æ“ä½œ**ï¼šæ”¯æŒæ‰¹é‡æ¨¡å‹ç®¡ç†
- **é«˜çº§åŠŸèƒ½**ï¼šä»»åŠ¡é¢„åŠ è½½ã€è‡ªåŠ¨ç®¡ç†é…ç½®

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
GPUæ¨¡å‹ç®¡ç†ç³»ç»Ÿ
â”œâ”€â”€ æ ¸å¿ƒç»„ä»¶
â”‚   â”œâ”€â”€ GPUModelManager      # æ¨¡å‹ç®¡ç†å™¨
â”‚   â”œâ”€â”€ ModelLifecycleManager # ç”Ÿå‘½å‘¨æœŸç®¡ç†
â”‚   â””â”€â”€ GPUManagerInitializer # ç³»ç»Ÿåˆå§‹åŒ–
â”œâ”€â”€ å·¥å…·æ¨¡å—
â”‚   â”œâ”€â”€ GPUUtils            # GPUå·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ MemoryMonitor       # å†…å­˜ç›‘æ§
â”‚   â””â”€â”€ ErrorHandler       # é”™è¯¯å¤„ç†
â””â”€â”€ ç•Œé¢ç»„ä»¶
    â””â”€â”€ ModelManagementTab  # Gradioç®¡ç†ç•Œé¢
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç³»ç»Ÿåˆå§‹åŒ–

```python
from src.core.gpu_manager_init import initialize_gpu_management

# åˆå§‹åŒ–GPUæ¨¡å‹ç®¡ç†ç³»ç»Ÿ
success = initialize_gpu_management()
if success:
    print("GPUæ¨¡å‹ç®¡ç†ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
```

### 2. åŠ è½½æ¨¡å‹

```python
from src.core.gpu_model_manager import get_model_manager

manager = get_model_manager()

# åŠ è½½DDSP-SVCæ¨¡å‹
ddsp_model_id = manager.load_ddsp_model(
    model_path="path/to/ddsp_model.pth",
    device="auto"  # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è®¾å¤‡
)

# åŠ è½½IndexTTSæ¨¡å‹
tts_model_id = manager.load_index_tts_model(
    model_dir="path/to/index_tts_model",
    device="cuda:0"
)
```

### 3. ä½¿ç”¨æ¨¡å‹

```python
# è·å–æ¨¡å‹å®ä¾‹
ddsp_model = manager.get_model(ddsp_model_id)
tts_model = manager.get_model(tts_model_id)

# ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†
if ddsp_model:
    result = ddsp_model.infer(audio_data, speaker_id=1)

if tts_model:
    result = tts_model.infer(text="Hello", speaker_audio="speaker.wav")
```

### 4. å¸è½½æ¨¡å‹

```python
# å¸è½½å•ä¸ªæ¨¡å‹
manager.unload_model(ddsp_model_id)

# å¸è½½æ‰€æœ‰æ¨¡å‹
manager.unload_all_models()
```

## ğŸ“Š å†…å­˜ç›‘æ§

### å¯åŠ¨ç›‘æ§

```python
from src.utils.memory_monitor import start_global_monitoring

# å¯åŠ¨å…¨å±€å†…å­˜ç›‘æ§
start_global_monitoring()
```

### è·å–çŠ¶æ€

```python
from src.utils.memory_monitor import get_memory_monitor

monitor = get_memory_monitor()

# è·å–å½“å‰çŠ¶æ€
status = monitor.get_current_status()
print(f"GPUå†…å­˜ä½¿ç”¨: {status['gpu_memory']}")
print(f"ç³»ç»Ÿå†…å­˜ä½¿ç”¨: {status['system_memory']}")

# è·å–å†…å­˜å†å²
history = monitor.get_memory_history(minutes=10)
```

## ğŸ”„ ç”Ÿå‘½å‘¨æœŸç®¡ç†

### ä»»åŠ¡é…ç½®

```python
from src.core.model_lifecycle import get_lifecycle_manager, TaskProfile, PreloadConfig, ModelType

lifecycle = get_lifecycle_manager()

# å®šä¹‰è¯­éŸ³åˆæˆä»»åŠ¡
task_profile = TaskProfile(
    task_name="voice_synthesis",
    required_models=[
        PreloadConfig(
            model_type=ModelType.INDEX_TTS,
            model_path="path/to/index_tts",
            device="auto",
            priority=1
        )
    ],
    memory_limit_mb=4096,
    auto_cleanup=True
)

# æ³¨å†Œä»»åŠ¡é…ç½®
lifecycle.register_task_profile(task_profile)
```

### ä»»åŠ¡é¢„åŠ è½½

```python
# ä¸ºä»»åŠ¡é¢„åŠ è½½æ¨¡å‹
results = lifecycle.preload_for_task("voice_synthesis")
print(f"é¢„åŠ è½½ç»“æœ: {results}")

# ä»»åŠ¡å®Œæˆåæ¸…ç†
cleanup_results = lifecycle.cleanup_for_task("voice_synthesis")
print(f"æ¸…ç†ç»“æœ: {cleanup_results}")
```

## ğŸ›ï¸ Webç•Œé¢ä½¿ç”¨

### å¯åŠ¨åº”ç”¨

```bash
python main.py
```

### è®¿é—®GPUç®¡ç†ç•Œé¢

1. æ‰“å¼€æµè§ˆå™¨è®¿é—® `http://localhost:7860`
2. ç‚¹å‡» "ğŸ”§ GPUæ¨¡å‹ç®¡ç†" æ ‡ç­¾é¡µ
3. ä½¿ç”¨ç•Œé¢è¿›è¡Œæ¨¡å‹ç®¡ç†æ“ä½œ

### ç•Œé¢åŠŸèƒ½

- **æ¨¡å‹æ§åˆ¶**ï¼šåŠ è½½/å¸è½½æ¨¡å‹
- **çŠ¶æ€ç›‘æ§**ï¼šæŸ¥çœ‹GPUå’Œå†…å­˜çŠ¶æ€
- **æ¨¡å‹åˆ—è¡¨**ï¼šæŸ¥çœ‹å·²åŠ è½½æ¨¡å‹è¯¦æƒ…
- **GPUç›‘æ§**ï¼šå®æ—¶GPUä¿¡æ¯æ˜¾ç¤º
- **é«˜çº§åŠŸèƒ½**ï¼šä»»åŠ¡é¢„åŠ è½½ã€è‡ªåŠ¨ç®¡ç†è®¾ç½®

## âš™ï¸ é…ç½®é€‰é¡¹

### GPUè®¾ç½®

åœ¨ `config.yaml` ä¸­é…ç½®GPUç®¡ç†å‚æ•°ï¼š

```yaml
gpu:
  auto_cleanup_enabled: true
  cleanup_threshold_percent: 85.0
  idle_timeout_minutes: 30
  memory_monitoring_enabled: true
  memory_monitoring_interval: 2.0
  max_memory_usage_percent: 90.0
```

### ç¯å¢ƒå˜é‡

```bash
# è®¾ç½®è®¾å¤‡
export CREATYOURVOICE_DEVICE=cuda:0

# å¯ç”¨è°ƒè¯•æ¨¡å¼
export CREATYOURVOICE_DEBUG=true
```

## ğŸ”§ å‘½ä»¤è¡Œå·¥å…·

### æ£€æŸ¥GPUçŠ¶æ€

```bash
python main.py gpu
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
=== GPUæ¨¡å‹ç®¡ç†çŠ¶æ€ ===
ç³»ç»Ÿåˆå§‹åŒ–: âœ“
å·²åŠ è½½æ¨¡å‹: 2/3
è‡ªåŠ¨æ¸…ç†: å¯ç”¨
å†…å­˜ç›‘æ§: è¿è¡Œä¸­
ç”Ÿå‘½å‘¨æœŸç®¡ç†: å¯ç”¨

=== GPUè®¾å¤‡ä¿¡æ¯ ===
GPU 0: NVIDIA GeForce RTX 4090
  å†…å­˜: 2048MB / 24576MB (8.3%)
  åˆ©ç”¨ç‡: 15.2%
```

## ğŸ› ï¸ é«˜çº§åŠŸèƒ½

### è‡ªå®šä¹‰æ¢å¤ç­–ç•¥

```python
from src.utils.error_handler import get_error_handler, RecoveryAction, RecoveryStrategy

error_handler = get_error_handler()

# æ³¨å†Œè‡ªå®šä¹‰æ¢å¤åŠ¨ä½œ
def custom_recovery():
    # è‡ªå®šä¹‰æ¢å¤é€»è¾‘
    return True

error_handler.register_recovery_action(
    "CustomError",
    RecoveryAction(
        strategy=RecoveryStrategy.RETRY,
        action=custom_recovery,
        description="è‡ªå®šä¹‰æ¢å¤ç­–ç•¥",
        max_attempts=3
    )
)
```

### å†…å­˜è­¦å‘Šå›è°ƒ

```python
from src.utils.memory_monitor import get_memory_monitor

def memory_alert_callback(alert):
    print(f"å†…å­˜è­¦å‘Š: {alert.message}")
    # è‡ªå®šä¹‰å¤„ç†é€»è¾‘

monitor = get_memory_monitor()
monitor.add_alert_callback(memory_alert_callback)
```

### æ™ºèƒ½æ¨¡å‹åˆ‡æ¢

```python
from src.core.gpu_manager_init import auto_load_for_synthesis, auto_load_for_conversion

# ä¸ºè¯­éŸ³åˆæˆè‡ªåŠ¨åŠ è½½æ¨¡å‹
tts_model_id = auto_load_for_synthesis(
    text="Hello world",
    speaker_audio="speaker.wav"
)

# ä¸ºéŸ³è‰²è½¬æ¢è‡ªåŠ¨åŠ è½½æ¨¡å‹
ddsp_model_id = auto_load_for_conversion(
    audio_path="input.wav",
    target_speaker="speaker1"
)
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### å†…å­˜ä¼˜åŒ–å»ºè®®

1. **åˆç†è®¾ç½®æ¸…ç†é˜ˆå€¼**ï¼šæ ¹æ®GPUå†…å­˜å¤§å°è°ƒæ•´ `cleanup_threshold_percent`
2. **ä½¿ç”¨ä»»åŠ¡é¢„åŠ è½½**ï¼šä¸ºå¸¸ç”¨ä»»åŠ¡é¢„åŠ è½½æ¨¡å‹å‡å°‘ç­‰å¾…æ—¶é—´
3. **ç›‘æ§å†…å­˜ä½¿ç”¨**ï¼šå®šæœŸæ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µï¼ŒåŠæ—¶æ¸…ç†
4. **æ‰¹é‡æ“ä½œ**ï¼šé¿å…é¢‘ç¹çš„å•ä¸ªæ¨¡å‹åŠ è½½/å¸è½½

### è®¾å¤‡é€‰æ‹©ç­–ç•¥

```python
from src.utils.gpu_utils import GPUUtils

# è·å–æœ€ä¼˜è®¾å¤‡
optimal_device = GPUUtils.get_optimal_device()

# æ£€æŸ¥å†…å­˜éœ€æ±‚
if GPUUtils.check_memory_requirement(2048, optimal_device):
    # å†…å­˜å……è¶³ï¼Œä½¿ç”¨GPU
    device = optimal_device
else:
    # å†…å­˜ä¸è¶³ï¼Œä½¿ç”¨CPU
    device = "cpu"
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **CUDAä¸å¯ç”¨**
   - æ£€æŸ¥CUDAå®‰è£…ï¼š`nvidia-smi`
   - æ£€æŸ¥PyTorch CUDAæ”¯æŒï¼š`torch.cuda.is_available()`

2. **å†…å­˜ä¸è¶³**
   - é™ä½æ¨¡å‹å¹¶å‘æ•°é‡
   - å¢åŠ æ¸…ç†é¢‘ç‡
   - ä½¿ç”¨CPUæ¨¡å¼

3. **æ¨¡å‹åŠ è½½å¤±è´¥**
   - æ£€æŸ¥æ¨¡å‹æ–‡ä»¶è·¯å¾„
   - éªŒè¯æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§
   - æŸ¥çœ‹é”™è¯¯æ—¥å¿—

### è°ƒè¯•æ¨¡å¼

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
python main.py --log-level DEBUG

# æ£€æŸ¥ç³»ç»ŸçŠ¶æ€
python main.py check
```

## ğŸ“š APIå‚è€ƒ

### GPUModelManager

ä¸»è¦æ–¹æ³•ï¼š
- `load_ddsp_model(model_path, device, model_id)` - åŠ è½½DDSPæ¨¡å‹
- `load_index_tts_model(model_dir, device, model_id)` - åŠ è½½IndexTTSæ¨¡å‹
- `unload_model(model_id, force)` - å¸è½½æ¨¡å‹
- `get_model(model_id)` - è·å–æ¨¡å‹å®ä¾‹
- `optimize_memory()` - ä¼˜åŒ–å†…å­˜ä½¿ç”¨

### MemoryMonitor

ä¸»è¦æ–¹æ³•ï¼š
- `start_monitoring()` - å¯åŠ¨ç›‘æ§
- `stop_monitoring()` - åœæ­¢ç›‘æ§
- `get_current_status()` - è·å–å½“å‰çŠ¶æ€
- `get_memory_history(minutes)` - è·å–å†å²è®°å½•

### ModelLifecycleManager

ä¸»è¦æ–¹æ³•ï¼š
- `register_task_profile(profile)` - æ³¨å†Œä»»åŠ¡é…ç½®
- `preload_for_task(task_name)` - ä»»åŠ¡é¢„åŠ è½½
- `cleanup_for_task(task_name)` - ä»»åŠ¡æ¸…ç†
- `start_auto_management()` - å¯åŠ¨è‡ªåŠ¨ç®¡ç†

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Fork é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. æ¨é€åˆ°åˆ†æ”¯
5. åˆ›å»º Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ™ è‡´è°¢

æ„Ÿè°¢æ‰€æœ‰ä¸ºGPUæ¨¡å‹ç®¡ç†ç³»ç»Ÿåšå‡ºè´¡çŒ®çš„å¼€å‘è€…å’Œç”¨æˆ·ã€‚

---

**æ³¨æ„**ï¼šGPUæ¨¡å‹ç®¡ç†ç³»ç»Ÿéœ€è¦NVIDIA GPUå’ŒCUDAæ”¯æŒæ‰èƒ½å‘æŒ¥æœ€ä½³æ€§èƒ½ã€‚åœ¨CPUæ¨¡å¼ä¸‹ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½å—é™ã€‚
